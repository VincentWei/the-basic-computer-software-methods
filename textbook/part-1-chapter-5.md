# 第 5 章  多媒体：图像、图形及音视频  

早期的计算机主要通过文字和用户交互。比如，用户使用键盘输入命令或者编写程序，计算机输出文字或符号到屏幕上展示结果。自图形用户界面（GUI）被发明以来，人们可以通过屏幕看到图像、图形，甚至可以欣赏音乐，观看视频，玩游戏等等，极大拓宽了计算机的用途。在计算机技术中，除文字之外的这类信息类型被称为“多媒体（Multi-media）”。本章讲述计算机如何表述多媒体信息。  

## 5.1  计算机输出设备  

作为一种电子计算和通信工具，现代的个人计算机（包括我们现在日常使用的智能手机）和用户产生交互时，主要通过人类的视觉和听觉进行，因此其输出设备可划分为视觉输出设备和听觉输出设备两类。除此之外，智能手机也可以产生振动，尽管这种振动在智能手机中仅起到辅助作用，但仍然拓宽了计算机和用户之间的交互途径。在苹果公司 2015 年发布的智能手表产品中，根据不同的场景，智能手表会产生不同方式的振动，从而使得这种交互方式变成了除视觉、听觉之外的另外一种主要交互方式。相信在不远的将来，利用人类味觉、嗅觉等的新式交互方式会得到广泛应用，比如在智能机器人、虚拟现实等新型的计算机产品形态中。  

本节将简要阐述计算机视觉、听觉输出设备的硬件工作原理。  

### 5.1.1  计算机视觉输出设备  

从计算机程序的角度看，计算机的输出设备有两类。一类称为字符输出设备，一类称为图形输出设备。
字符输出设备出现在计算机的早期发展过程中。那时，人们通过一个个带有键盘和 CRT 屏幕（阴极射线管）的终端通过串口连接到单个大型机（mainframe）上输入程序，执行程序并观察程序的执行结果。人们在键盘上输出的字符会立即显示在屏幕上，而程序的输出则会连续不断地显示在屏幕上；当屏幕上的字符显示满屏之后会自动滚屏显示新的字符。  

字符输出设备为程序提供了非常简单的接口，在 C 语言中，我们调用 printf 函数，即可向屏幕上输出字符；调用 gets 函数，可等待并获得用户的输出。  

与字符输出设备不同的是图形输出设备。图形输出设备提供给程序在计算机屏幕上使用不同的颜色绘制任意形状的能力。此时，程序通过操作我们俗称为“显存（Video RAM）”的内存区域来控制屏幕上的每个像素点（Pixel），从而显示文字、绘制图形。  

尽管我们现在使用的个人电脑之显示屏均为图形显示设备，但字符输出设备仍然以直接或者间接的方式存在于我们的电脑上。比如，当我们打开个人电脑时，个人电脑的 BIOS 系统首先会将电脑的显示器以字符设备方式工作，显示一些有关 BIOS 版本、内存大小等信息，然后是 Windows 系统，此时，显示屏会以图形输出设备的方式工作，并显示图形用户界面。在 Windows 系统中，如果我们运行 cmd 命令，将打开字符终端模拟程序，此时，我们可以键入 DOS 命令，而其中的命令输出则会显示在字符终端模拟程序的窗口中。再比如，我们在 Linux 或者 Windows 系统中使用 telnet、SecureCRT 等登录到远程主机上时，运行其中的程序也以字符方式展示输出，而 telnet、SecureCRT 程序为这些运行在远程主机上的程序提供了一个模拟的字符输出设备。  

本质上，字符输出设备是任何计算机系统的最基础设施，在完整的计算机系统中，字符输出设备和当前我们常见的图形输出设备共存。显然，字符输出设备主要处理的是文本的显示，而对我们本章所讲的多媒体信息中的图像、图形相关信息，则必须使用图形输出设备。  

和计算机图形输出设备关联的计算机系统组件主要是显示卡（显示芯片）和显示器。以 PC 为例，显示卡（display adapter）是一块插在PC主机的电路板（或者集成在 PC 主板上）。一般显示卡由寄存器、存储器（Video RAM和ROM BIOS）、控制电路三大部分组成。大部分的图形显示卡都以兼容标准的VGA模式为基础。而在许多便携式智能设备上，用户面对的显示硬件通常是屏幕较小的 LCD 和 LCD 控制器（LCD controller）。  

不管是 PC 的显示卡还是嵌入式设备的 LCD 控制器，都将有一个显示RAM（video RAM，VRAM）区域，代表了要在屏幕上显示的图像。VRAM必须足够大，以处理显示屏幕上所有的像素。程序通过直接或间接地存取 VRAM 中的数据来进行图形操作，改变屏幕上的显示。许多显示硬件提供从 CPU 的地址和数据总线直接访问 VRAM 的能力，这相当于把 VRAM 映射到了CPU的地址空间，从而允许更快的 VRAM 访问速度。VRAM 组成的内存段，就叫帧缓冲区（Frame Buffer）。  

通常，CRT 显示器和 LCD 显示屏都是栅格设备，屏幕上的每一点是一个像素，整个显示屏幕就是一个像素矩阵。帧缓冲区中的数据按照显示器的显示模式进行存储，记录了显示屏幕上每一个像素点的颜色值，即像素值。  

我们知道，计算机以二进制方式存储数据，每位有两种状态（0 与 1）。对于单色显示模式，屏幕上一个像素点的颜色值只需用帧缓冲区中的一位表示，该位为 1 则表示该点是亮点。而在彩色显示模式下，要表示屏幕上像素点的颜色信息，需要更多的位或字节。比如，对于 16 色显示模式，就需 4 位来存储一个颜色值。在 256 色显示模式下，一个像素点占 8 位即一个字节。在 16 位色彩色显示模式下，则需要两个字节来存储一个像素点的颜色值。随着计算机硬件处理能力的提升以及 32 位系统的普及，现在不管是 PC 还是智能设备，基本上均采用 32 位色或 16 位色显示模式，其他的显示模式已经非常少见了。显然，显示一个像素点使用的位数越多，可显示的颜色越多，显示出来的色彩就越丰富，越可能接近真实世界[^1]。  

### 5.1.2  计算机听觉输出设备  

计算机听觉输出设备相对来讲比较简单，和收音机、录音机等使用的扬声器（俗称喇叭）或者耳机并无二致。音频（audio）信号通过电磁、压电或者静电效应，使得扬声器的纸盆或耳机膜片振动挤压空气，从而发出声音。  

当然，计算机中存储的音频信号都是数字化的，所以，和传统的收音机不同，计算机需要将数字化的音频信号通过 DA（数字到模拟）转换器转换成模拟的音频信号，然后再传输给扬声器发出声音。反过来，如果要让计算机录制声音，则需要经过 AD（模拟转数字）转换器将模拟的音频信号（由麦克风拾取）做数字化处理并存储为二进制数据。  

随着科技的发展，新近又出现了一种骨传导耳机。骨传导耳机直接通过人类头骨、颌骨将声音传导到听觉神经，而不经过耳朵和鼓膜。因此，骨传导耳机不再需要纸盆或者膜片。骨传导耳机可用于听觉有障碍的人群，如老人或耳聋患者。另外对正常人来讲，使用骨传导耳机时，同时也可以听到通过耳朵传入的声音，因此，可以提高佩戴耳机行进过程中的安全性。还有一个好处就是，骨传导耳机不会影响身边的其他人。相信骨传导耳机在未来的虚拟现实、增强现实产品中会有较为广泛的应用，也会逐步替代现有的耳机产品。  

## 5.2  图像  

### 5.2.1 位图及相关概念  

和图形输出设备的显存对应，在计算机程序中，我们使用位图（Bitmap；在某些系统中也称为像素图，Pixelmap）的概念来表示一副静态的图像。  

我们可以将位图理解成一个二维数组，其中记录了图像中每一个点的颜色信息，这些点称为像素（Pixel）。位图的高度和宽度以像素个数为单位表示。在电子计算机中，图形显示设备（CRT 或 LCD）普遍使用红绿蓝（RBG）三原色来表示一个像素的颜色。RGB 三种颜色分量的取值范围均为 0x00~0xFF，这样，理论上可表达的颜色种类有 224 之多，大大超出人眼能够分辨的颜色数量。假如我们用三个字节来表示一个像素，则高度和宽度分别为 256 的位图，需要 256\*256\*3 = 192KB 字节。  

除了位图的宽度、高度等信息外，还有一些和位图相关的概念，综述如下：  

- 每像素位数（bits per pixel），也称颜色深度（color depth），简称色深。色深表示位图中每个像素所占的位数。通常来讲，位图的色深可取 1、2、4、8、16、24 等，分别对应于单色、4 色、16 色、256 色、64K 色和 16M 色。同一位图中的各像素位数都相同，这样，使用不同的颜色深度，可表示的颜色数不同，存储每个像素所需要的空间也就不同。  
- 每像素字节数（bytes per pixel），通常表示为 bpp。bpp 表示每个像素所占的字节数。当然，这个概念仅仅出现在颜色深度大于等于 8 的情况下。当颜色深度为8时，bpp 为 1；颜色深度为 16 时，bpp 为 2；颜色深度为 24 时，bpp 为 3。需要注意的是，色深为 24 时，在 32 位系统上，为提高处理速度，每个像素会以四个字节为单位进行存储，这样，一次读取一个 32 位的整数即可获得这个像素的像素值。  
- 位图每行所占的字节数（bytes per line，也称为 pitch）。pitch 表示位图每行像素所占的字节数。通常来讲，位图的 pitch 一般被圆整为四的倍数。这样，在 32 位系统中装载位图数据之后，可确保位图的每行像素均对齐于四字节边界，便于应用程序优化处理位图像素值。  
- 位图的透明像素。某些图像格式（如 GIF），可定义透明色，当程序显示定义有透明色的位图时，要跳过透明像素，而和透明像素不同的像素应该覆盖已有的像素，这样就实现了透明的图片。  
- 像素的Alpha值。某些图像格式（如 PNG），可为每个像素点定义 Alpha 值。Alpha 值将参与位图像素值和要覆盖的像素值（已有像素值）之间的运算，使得最终的像素值是位图像素值和已有像素值以某个百分比混合后的像素值。通常，Alpha 值取 0 到 255 的整数值。当 Alpha 为 128 时，最终的像素值将是位图像素值和最终像素值各取一半之后的像素值，这样就实现了半透明效果。随 Alpha 值的不同，混合的效果不同；Alpha 为零时，将忽略位图像素值，而 Alpha 为 255 时，结果像素值就是位图像素值。如果位图中使用逐点 Alpha 值（即每个像素点都具有不同的 Alpha 值），则一半使用 32 位来表示一个像素，R、G、B 各占8位，Alpha 值占 8 位。  

在早期的个人电脑系统中，用于存储像素的内存相对比较昂贵，且受内存寻址空间大小的影响，使用每个像素均以 RGB 三原色分量来表示的方法会消耗很大的内存。比如上面提到的 129KB 字节的位图，对只有 640KB 可用内存的 DOS 系统来说，显然过大。于是，人们使用其他变通的方法来表达位图，其中最有效的方法使用调色板技术。  

使用调色板技术时，每个像素的颜色值不再包含直接的 RGB 分量信息，而是一个编号，使用该编号查找一个线性数组，即可获得这个像素对应的真实 RGB 颜色分量，而这个线性数组就是调色板。如图 5.1 所示。  

<div align="center">
  ![位图及其调色板](illustration/img-5-1.png)<br>
  图 5-1 位图及其调色板
</div>
<br>

通常，调色板由一个足够大的 RGB 数组表示，该数组每个成员含有三个字节，分别表示R、G、B 三种颜色分量。显示位图时，并不能直接使用像素值来表示 RGB 三种颜色，而是首先要以像素值为索引在调色板中查找对应的 RGB 三种分量的值，最终以这个值作为实际的颜色显示出来。当颜色深度为 1 时，调色板通常只有两个入口，分别表示像素值为0和1时的 RGB 颜色分量；而颜色深度为 2 时，调色板通常有四个入口，分别表示像素值为 0、1、2、3 时的 RGB 分量。以此类推，颜色深度为 8 时，调色板通常有 256 个入口。  

当位图的颜色深度取 16、24 时，就不再采用调色板，而用直接使用像素值来表示颜色分量。比如在色深为 24 位的位图中，每个像素值存储的是直接的 RGB 分量数据，每个分量占 8 位；而在 16 位位图中，RGB 分量分别占5位、6 位和 5 位。如图 5.2 所示。

<div align="center">
  ![十六位色像素和 RGB 分量](illustration/img-5-2.png)<br>
  图 5-2 十六位色像素和 RGB 分量
</div>
<br>

在计算机中，将一个静态图像显示显示到屏幕上的过程，大致经历如下几个步骤：  

1) 从图像文件中获得有关图像大小、调色板等信息。  
2) 按照图像文件的存储格式，解码图片，此时在计算机内存中用来表示图像的位图和当前计算机系统使用的具体显示模式无关，因此被称为设备无关的位图（Device-Indepdent Bitmap）。  
3) 将这个位图中的每个像素转换为适配计算机系统当前显示模式像素值，形成设备相关位图（Device-Depedent Bitmap）。  
4) 将设备相关位图逐点、逐行或者整个复制到图形显示设备帧缓冲区的指定内存位置或区域。  

为了更好地理解设备相关位图和设备无关位图的概念，假设我们有一个使用调色板表示的 256 色位图，要在 16 位色的屏幕上显示。装载调色板以及对应的像素值到内存中时，由调色板和表示图像各像素点的数组表示的位图就是设备无关的。而要显示到屏幕上，我们需要将这个位图中的每个像素点转换为 16 位色形式，即根据设备无关的位图像素值查找调色板，然后将对应的调色板颜色 RGB 分量组装成图 5-2 所示的 16 位色像素值，然后复制到显存中。执行这种像素转换之后的位图就称为设备相关位图。  

### 5.2.2 色彩空间  

本章目前为止提到的是 RGB 色彩空间（color space），即通过红、绿、蓝三原色的不同取值来确定一个颜色。除了 RGB 色彩空间之外，业界还使用 YUV、YIQ、CMYK 等其他色彩空间来定义颜色。其中 YUV 色彩空间主要用于电视视频信号的传输，可兼容老式的黑白电视视频信号。YUV 色彩空间中的 Y 指亮度（luminance），即灰阶值（黑白电视使用灰阶值定义），而 U 和 V 表示色调和饱和度，用于定义像素的颜色，即色度（chrominance）。“亮度”通过 RGB 输入信号建立，方法是将 RGB 信号的特定部分叠加到一起。“色度”则定义了颜色的两个方面：色调与饱和度，分别用和 U（Cb）和 V（Cr）来表示。其中，U（Cb）反映的是 RGB 输入信号蓝色部分与 RGB 信号亮度值之间的差异；而V（Cr）反映了 RGB 输入信号红色部分与 RGB 信号亮度值之间的差异。RGB 到 YUV 色彩空间一般具有如下的转换公式：  


<div align="center">
<img src="https://latex.codecogs.com/svg.image?{\color{Magenta}Y=0.30R&plus;0.59G&plus;0.11B&space;}" /><br>
<img src="https://latex.codecogs.com/svg.image?{\color{Magenta}U=(B-Y)\times&space;0.493&space;}" /><br>
<img src="https://latex.codecogs.com/svg.image?{\color{Magenta}V=(R-Y)\times&space;0.877&space;}" /><br>
</div>
<br>

类似地，在美式电视信号标准 NTSC 中，还使用 YIQ 色彩空间，对应的颜色空间转换公式为：

<div align="center">
<img src="https://latex.codecogs.com/svg.image?{\color{Magenta}Y=0.30R&plus;0.59G&plus;0.11B&space;}" /><br>
<img src="https://latex.codecogs.com/svg.image?{\color{Magenta}I=0.60R-0.28G-0.32B&space;}" /><br>
<img src="https://latex.codecogs.com/svg.image?{\color{Magenta}Q=0.21R-0.52G&plus;0.31B&space;}" /><br>
</div>
<br>

另外，CMYK 色彩空间主要用于印刷业，通过青、洋红、黄和黑四种颜色的不同取值来定义一种颜色。RGB 色彩空间适合于背景为黑色的显示设施，而 CMYK 适合于背景为白色的显示设施。需要注意的是，色彩空间之间可以转换，但严格意义上讲，不同的色彩空间并不重叠，也就是说，不同的色彩空间中的颜色并不是一一对应的，因此色彩在不同的色彩空间转换时，会带来一些失真。  

### 5.2.3 流行的图像格式  

上个小节提到的位图，本质上指计算机在显示一个图像（Image）时，在内存中表示这个图像的方法。我们知道，我们让计算机显示一副图片时，对应的操作是打开这个图片文件。比如我们使用数码相机或者智能手机拍摄的照片文件，通常使用 JPG 后缀名，而常见的图标文件，则具有 PNG、GIF 等后缀名。这些文件，就是我们本节要讲述的图像文件格式。  

图像文件本质上定义的是一个设备无关的位图，包括其高度、宽度以及像素值等信息。为了有效降低存储图像所使用的存储空间，不同的图像文件格式会定义自己特有的编码方式。这些编码方式本质上属于压缩算法。在一副图像中，我们经常会发现有很多相邻的像素具有同样的像素值，比如白色背景上显示一个黑点的图像。针对这种情形，我们就可以使用特定的压缩算法来降低最终图片文件的存储大小，从而节省存储空间和传输带宽。  

图像的压缩算法又可以分为无损压缩和有损压缩两类。前者表示解压后的位图和原色位图一样，不损失任何像素值信息；后者表示解压后的位图和原始位图不同，会损失原始位图中的像素信息。用于照片的 JPEG 图片格式，使用有损压缩算法，在某些情况下，同一张照片使用 JPEG 格式存储，选择不同的压缩比，其大小可能有十倍之差，粗略观察，人眼可能无法有效分辨其内容的差别。有损压缩虽然会损失数据的精度，但可提高压缩比，非常适合处理图像、音频、视频等多媒体数据。  

#### *1) GIF*

GIF（Graphics Interchange Format）格式。这种格式采用了一种非常适合于图像数据的压缩算法（LZW算法），但只能用于 256 色位图。GIF 格式采用调色板定义各个像素值对应的 RGB 分量。GIF 格式有两种规范，GIF87a 和 GIF89a。GIF89a 在 GIF87a 规范基础上定义了简单的动画效果，且可定义透明色。目前，我们在微博、微信等应用上看到的一些动画图片（俗称“动图”）或者动画表情，使用的是 GIF89a 定义的 GIF 图片格式。  

GIF 图片文件以 gif 为后缀名，二进制方式存储，其内容依次分为四个区块：  

1) 签名及版本信息。GIF 文件的前三个字符始终为“GIF”，紧接着是“87a”或者“89a”这三个字符。这种设计有两个好处。第一，便于程序识别文件格式；第二，用于判断 GIF 文件格式所遵循的规范（或版本）。如果是 89a，则表示包含有多张图片，可用于实现动画。  
2) 全局描述信息。其中包含 GIF 图片的宽度（16位无符号整数，小头）、高度（16位无符号整数，小头）、色深（用于确定调色板大小，可能为1、4、8）、背景色在调色板中的索引值（0~256）、全局调色板（RGB 三原色数组）。  
3) 扩展区。扩展区以字符“c”打头，用来定义文本扩展信息（字符串）、应用扩展信息（字符串）、注释扩展信息（字符串），以及 89a 格式的扩展属性，比如动画各帧之间的延迟时间（以 10ms为单位）、是否需要用户输入（点击鼠标或者按下回车键）才开始动画、是否透明等。  
4) 图片描述区。对于动画 GIF，可包含多个描述区。每个描述区以字符“;”打头，其后九个字节是头部信息。其中包含当前帧在整个图片中的偏移位置以及大小，最后一个字节含有多重信息，如这一帧图像是否使用全局调色板以及是否交错存储；若不使用全局调色板，则这个字节中还包含有色深信息，其后便是这一帧图像使用的局部调色板。头部信息之后，包含的是由 LZW 算法编码的图像数据。  

GIF 使用的 LZW 算法本质上是一种通用无损压缩算法，将在本书第三篇“信息的计算机处理”中讲述。交错（interlace）存储情况下，图片中首先保存序号为偶数的水平扫描线像素数据，然后是序号为奇数的水平线像素数据。这种设计，使得我们可以首先看到图片的整体样子，然后再看到完整全貌，这在传输速度比较低的情形下尤其有用。当然，现在网络和磁盘访问速度都很高了，这么做已经没有多大意义了。
尽管一张 GIF 只能描述 256 种颜色，但这对表情符号来说足够了。对于颜色较为丰富的图片，使用 GIF 格式来存储时，一般通过统计不同颜色的像素数量来确定调色板。对颜色特别丰富的图片，需要做一些近似处理才能生成比较接近原始图的 GIF 图片，当然，这个时候生成的 GIF 图片会出现色彩失真的情形。在互联网上，某些视屏片段会被转换成 GIF 动画格式，以方便传输和展现。色彩失真就会出现在这种情况下。  

在使用 GIF 格式存储颜色丰富的图片时，为了能更加准确地反映真实颜色，生成 GIF 图片的程序还可以使用一些算法来弥补颜色损失。其中一种常见算法称为“抖动（Dither）”。  

如图 5-3 所示[^2]。最后一幅图使用 24 位色表示一副渐变图像（如日落时的天空），由于 24 位色情况下的颜色空间足够大，可以真实体现出这幅图的细节部分；如果将这幅图保存为 GIF 格式，则会损失大量的颜色信息，如果不做任何处理，将出现色彩带（Color Banding），即第一幅图所示情形；第二幅图使用抖动算法，用小的混合色块来模拟颜色的渐变过程，从而可以在使用色深为 8 位的 GIF 格式之情形下，仍然能够获得较为接近真实情况的图片。  

<div align="center">
  ![色带及抖动处理](illustration/img-5-3.png)<br>
  图 5-3  色带及抖动处理
</div>
<br>

抖动处理的原理很简单，其目的是使用调色板中的可用颜色的扩散来获得近似效果，此时人眼会自动将扩散的不同颜色混合成新的颜色。如图 5-4 第一张图所示，当红色和蓝色的方块足够小时，图片将变成紫色；而第二张图则通过单色图来模拟出了灰度效果（如使用铅笔或者钢笔绘制的素描图一样）。  

<div align="center">
  ![抖动处理的原理](illustration/img-5-4.png)<br>
  图 5-4  抖动处理的原理
</div>
<br>          

#### *2) BMP*  

Windows BMP 格式。这是 Windows 系统定义的图像格式，可支持 4、8、16、24 等多种颜色深度，但不支持透明和 Alpha 混合。Windows BMP 格式针对 4 位色和 8 位色支持 RLE（Run Length Encode）编码方式。  

Windows BMP 文件的后缀名为 bmp，其文件格式和 GIF 类似，但不能定义多帧图像，更为简单些。一个 BMP 文件依次分如下几个数据区：  

1) 文件头部区域。该区域定义了 BMP 图片文件的签名、文件大小以及位图数据在文件中的偏移量。对应的 C 语言结构如下面的清单所示。其中 bfType 始终为 19778，即两个字符“BM”对应的 16 位无符号整数值。  

```c
typedef struct BITMAPFILEHEADER
{
   unsigned short bfType;
   unsigned long  bfSize;
   unsigned short bfReserved1;
   unsigned short bfReserved2;
   unsigned long  bfOffBits;
} BITMAPFILEHEADER;
```  

2) 位图头部区域。该区域定义了位图的基本信息，包括位图头部信息的字节长度、位图宽度、位图高度、位面数量、色深、位图数据编码方式、图片大小等信息。对应的 C 语言结构如下面的清单所示。其中编码方式有适用于 24 位色的 RGB 各占三个字节的情况，适合 15 位色、16 位色情形的 RGB 位域（Bit field）方式，以及适合 4 位色、8 位色的 RLE 编码方式。若色深小于等于 16 位色，则其后包含有调色板数组。  

```c
typedef struct WINBMPINFOHEADER
{
   unsigned long  biSize;
   unsigned long  biWidth;
   unsigned long  biHeight;
   unsigned short biPlanes;
   unsigned short biBitCount;
   unsigned long  biCompression;
   unsigned long  biSizeImage;
   unsigned long  biXPelsPerMeter;
   unsigned long  biYPelsPerMeter;
   unsigned long  biClrUsed;
   unsigned long  biClrImportant;
} WINBMPINFOHEADER;
```  

3) 位图数据。以水平扫描线为单位组织，每条扫描线占用的字节长度被圆整为 4 的倍数，以便优化装载过程。  

RLE（Run Length Encode）是一种非常简单的图像压缩方法。当一条扫描线上有多个像素值连续相同时，即可采用这种编码方法。以针对 8 位色的 RLE 编码为例，该编码首先给出的是其后具有相同颜色的像素数量，然后像素值。比如“0x02 0x00 0x04 0x66”解码后的实际像素值序列应该为“0x00 0x00 0x66 0x66 0x66 0x66”。  

#### *3) PNG*  

PNG（Portable Network Graphic Format，可移植网络图形格式），是 GNU 为了取代 GIF 格式[^3]而定义的图像格式，可支持各种颜色深度，同时支持透明和 Alpha 混合。PNG用来存储灰度图像时，灰度图像的色深最高可达16位，存储彩色图像时，彩色图像的色深可达 48 位，并且还可存储高达 16 位的 Alpha 通道数据。PNG使用从LZ77派生的无损数据压缩算法。  

PNG 是为替代 GIF 格式而生，同时也提供了超出 GIF 很多特性。比如，可以支持高于 8 位的色深，支持 256 级的 Alpha 混合效果。在使用调色板和 32 位色情形下，PNG 可使用 ARGB 颜色分量来表示每个像素值，从而可以定义每个像素独立的 Alpha 半透明混合效果。需要注意的是，PNG 不支持类似 GIF 89a 规范定义的动画效果。2004年末，APNG 规范提出了一个简单的 PNG 的动画扩展实现方案，但并没有流行开来，这主要是因为 GIF 相关的专利随后到期。  

相比前述的 GIF 和 BMP 格式，PNG 文件格式要复杂很多。PNG 文件的后缀名一般为 png，在程序中我们使用开源的 libpng[^4] 库来解码 PNG 格式文件或者保存图片为 PNG 格式。  

#### *4) JPEG*  

JPEG（Joint Photographic Experts Group，联合图像专家小组）是第一个国际图像压缩标准。使用这种压缩标准保存的图像文件可用来存储色彩非常丰富的图片。因为 JPEG 文件使用有损压缩技术存储像素值，数据压缩比要比 GIF、PNG 等使用无损压缩算法的图像格式高很多，而且其图像的还原度高，因此被广泛使用在数码相机、智能手机等消费类电子产品中。  

JPEG 图片文件通常的后缀名可能为jpeg、jfif、jpg 或 jpe，最为常见的是 jpg。其中 JFIF 是 JPEG File Interchange Format（JPEG 文件交换格式）的缩写，是对 JPEG 压缩图片的一种封装形式，常用于电脑和数码相机、智能手机产品。某些情况下，也使用 ExifJPEG 文件格式。  

JPEG 支持使用 RGB、YUV 色彩空间以及灰度色彩空间来定义像素，并使用了混合型的图像压缩编码方法，即同时使用有损压缩及无损压缩算法。相关的有损压缩及无损压缩算法，我们将在本书第三篇中讲述。
我们将原始的位图压缩生成 JPEG 文件时，可指定不同的压缩级别，通常为 11 级，以 0—10 级表示。其中 0 级压缩比最高，但图像品质最差；在采用细节几乎无损失的 10 级质量压缩保存时，压缩比也可达 5:1，此时，JPEG 不使用有损压缩算法来压缩图像，其结果类似 PNG。读者可以在 PC 上尝试将某个 BMP 格式保存的图片另存为 JPEG 格式，会发现最终生成的文件可能仅有几百KB。  

JPEG2000 作为 JPEG 的升级版，其压缩率比 JPEG 高约 30% 左右，同时支持有损压缩和无损压缩。JPEG2000 格式有一个极其重要的特征：它能实现渐进传输，即先传输图像的轮廓，然后逐步传输数据，不断提高图像质量，让图像由朦胧到清晰显示。此外，JPEG2000 还支持所谓的“感兴趣区域”特性，利用这一特性，可以指定图像上感兴趣区域具有较高的压缩质量，还可以选择指定的部分先解压缩。但遗憾的是，JPEG2000 并没有取代 JPEG 成为流行的图片压缩格式。  

在现代计算机系统中，由于 JPEG/JPEG2000 的压缩和解压过程相对复杂，使用软件方式时，压缩或者解压 JPEG/JPEG2000 要占用很大的 CPU 运算能力和内存，因此，JPEG/JPEG2000 的压缩和解压一般通过专门的硬件芯片来完成，尤其在数码相框和智能手机这类消费类电子产品中。在没有硬件支持 JPEG/JPEG2000 的压缩和/或解压的情形下，可使用 libjpeg[^5] 或者 OpenJPEG[^6] 开源软件来完成 JPEG/JPEG2000 的压缩或解压。  

#### *5) 其他图像格式*  

除了上面提到的流行图像格式之外，在计算机技术发展的过程中，还出现过如下一些图像格式：  

- PCX 是一种由美国 Zsoft 公司所开发的图像文件格式，原本是该公司的PC Paintbrush 软件的文件格式（PCX代表PC Paintbrush Exchange），却成了最广泛接受的 DOS 图像标准之一，然而随着 Windows 的普及，这个图像格式逐渐被 GIF、JPEG、PNG 等取代。  
- TIFF（Tagged Image File Format，标签图像文件格式）是一种灵活的位图格式，通过使用标签，可以在单个文件中定义多幅图像和数据，因此尤其适合于文档和书籍的扫描存储，因此，该文件格式也主要用于扫描仪和传真图像。最初，TIFF 只提供单色（二值）图像格式，采用 CCITT Group IV 2D压缩算法，但随着随着扫描仪功能愈来愈强大， TIFF 逐渐支持灰阶图像和彩色图像，且可以灵活定义图像的压缩算法，如 JPEG 或者 LZW 算法。  
- WebP 是一种同时提供了有损压缩与无损压缩的图片文件格式，派生自视频编码格式 VP8，是由 Google 在收购 On2 Technologies 后发展而来的图片格式，以 BSD 授权条款发布。WebP 最初在2010年发布，目标是减少文件大小，但达到和 JPEG 格式相同的图片质量，希望能够减少图片文件在网络上的传输时间。2011 年 11 月 8 日，Google 开始让 WebP 支持无损压缩和透明色的功能。根据 Google 较早的测试，WebP 的无损压缩比相同的 PNG 文件可以减少 28% 到 45% 的文件大小。  

## 5.3  二维矢量图形  

在介绍本节内容之前，我们首先区分一下图形和图像这两个计算机术语。在计算机技术中，图形（Graphics）泛指一切显示在计算机屏幕上的东西，可以是一副图片，也可以是绘制出来的按钮等形状；而图像（Image）指以像素点数组为组织形式的图形，一般指图片（Picture）。从概念上讲，图形是图像的超集，而图像又可以称为栅格化（Rasterized）图形。  

本节要讲述的矢量图形（Vectorized Graphics）指使用矢量化的方式来定义形状以及可能的填充方法在内的计算机图形表述方法。矢量图形区别于使用像素数组为形式的图形表述方法。相比图像而言，矢量图形具有如下优势：  

- 通过使用数学方法描述图形的轮廓和填充属性，我们可以随意放大或缩小矢量图形，不会出现放大图像时产生的马赛克效果。  
- 矢量化描述的图形比起特定大小的图像来讲，往往可以节省存储空间和传输带宽，加快传输速度。  

由于矢量图形技术的如上优势，在现代计算机系统中，显示文字时使用的字型通常使用矢量图形来描述，如 TrueType 字体格式（详情可阅本书第四篇“信息的计算机表达”）。另外，几年前流行的 Flash 动画，也是基于矢量图形的。  

当然，矢量图形的问题也很明显，比如：  

- 由于计算机的图形输出设备并不能直接处理矢量化图形，因此，程序需要首先将矢量图形进行栅格化（Rasterize）处理，转换为给定大小的位图，然后才能显示到屏幕上。但这需要较强的 CPU 或者 GPU 处理能力。  
- 在像素密度较小的显示设备上，经过处理的矢量图形会存在锯齿状等变形问题。这也是为什么很多 TrueType 字体文件中，针对小尺寸的字型直接内嵌有点阵字体的原因（见本书第四篇“信息的计算机展现”）。  

当前网络上最为流行的矢量图形格式是 SVG。SVG（Scalable Vector Graphics 可缩放矢量图形）是用于描述二维矢量图形的一种图形格式，采用可扩展标记语言（XML，见本篇第六章“抽象对象及复杂对象的表述”）。SVG 由 W3C 制定，是一个开放标准。SVG 主要支持以下几种图形对象：  

- 矢量图形对象，包括矩形、圆、椭圆、多边形、直线、任意曲线等。
- 嵌入式外部图形，包括P NG、JPEG 等外部图像以及外部 SVG 等。
- 文字对象。  

SVG 格式有如下优势：  

- SVG 可嵌入到 HTML 页面中，用来替代栅格图形，亦可嵌入 JavaScript 脚本来控制 SVG 对象，从而形成类似 Flash 的动画效果。  
- SVG可方便地创建文字索引，从而实现基于内容的图形搜索。  
- SVG 图形格式支持多种滤镜和特殊效果，在不改变图形内容的前提下可以实现位图格式中类似文字阴影的效果。  

清单 5-1 给出了一个简单的 SVG 示例文件[^7]。

<small>清单 5-1  SVG 示例</small>

```
<?xml version="1.0"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" 
    "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" version="1.1" 
    width="467" height="462">
  <!-- 红色矩形 -->
  <rect x="80" y="60" width="250" height="250" rx="20" fill="red"
         stroke="black" stroke-width="2px" />
  <!-- 蓝色矩形 -->
  <rect x="140" y="120" width="250" height="250" rx="40" fill="blue"
         stroke="black" stroke-width="2px" fill-opacity="0.7" />
</svg>
```  
<br>

上面的 SVG 文件定义了两个圆角矩形，边框颜色均为黑色，但一个用红色填充，一个用蓝色填充；蓝色矩形覆盖在红色矩形上，透明度为 0.7。该 SVG 的渲染效果见图 5-5。  

<div align="center">
  ![示例 SVG 文件的渲染效果](illustration/img-5-5.png)<br>
  图 5-5  示例 SVG 文件的渲染效果
</div>
<br>

除了上面的矩形（rect）之外，SVG 还定义有其他基本的形状，其中包括线段（line）、折线段（polyline）、圆（circle）、椭圆（ellipse）、多边形（polygon）等。而对复杂的二维图形，一般使用路径（Path）来描述。  

计算机图形学中的路径就是各种曲线段连接起来的复杂曲线，一般用来定义对象的边界。如果路径定义的图形是封闭的，还可以定义其中的颜色填充模式，如单纯的颜色或者渐变色。图 5-6 给出了一个典型的路径。  

<div align="center">
  ![路径](illustration/img-5-6.png)<br>
  图 5-6  路径
</div>
<br>

在 SVG 中，一个路径由四类线段组成，分别是直线段、二次贝塞尔曲线、三次贝塞尔曲线、圆弧或椭圆弧。图 5-6 中的路径，由直线和椭圆弧/圆弧组成，对应的 SVG 代码如清单 5-2 所示[^8]。  

<small>清单 5-2  SVG 路径示例</small>  

```
<?xml version="1.0" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" 
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg width="12cm" height="5.25cm" viewBox="0 0 1200 400"
     xmlns="http://www.w3.org/2000/svg" version="1.1">
  <title>定义路径</title>
  <desc>Picture of a pie chart with two pie wedges and
        a picture of a line with arc blips</desc>
  <rect x="1" y="1" width="1198" height="398"
        fill="none" stroke="blue" stroke-width="1" />

  <path d="M300,200 h-150 a150,150 0 1,0 150,-150 z"
        fill="red" stroke="blue" stroke-width="5" />
  <path d="M275,175 v-150 a150,150 0 0,0 -150,150 z"
        fill="yellow" stroke="blue" stroke-width="5" />

  <path d="M600,350 l 50,-25 
           a25,25 -30 0,1 50,-25 l 50,-25 
           a25,50 -30 0,1 50,-25 l 50,-25 
           a25,75 -30 0,1 50,-25 l 50,-25 
           a25,100 -30 0,1 50,-25 l 50,-25"
        fill="none" stroke="red" stroke-width="5"  />
</svg>
```  

其中，path 元素中的 d 属性给出了路径的生成数据，M、l、a 等参数前缀表示执行的绘制动作。大写的 M、L、A 等给出的坐标是绝对坐标，分别表示移动到给定点、绘制直线到给定点、绘制圆弧到给定点等等，而 l、a 等表示其后给出的坐标是相对坐标。  

和直线段由两个端点来描述类似，贝塞尔曲线用两个端点以及一个或两个个控制点来描述。图 5-7 给出了贝塞尔曲线的示例。  

<div align="center">
  ![贝塞尔曲线](illustration/img-5-7.png)<br>
  图 5-7  贝塞尔曲线
</div>
<br>

显然，借助基本形状和路径，计算机可以表述各种或简单或复杂的二维图形。  

在矢量图形中，对封闭形状，我们还需要定义其内部的填充模式。SVG 定义有三种填充模式：单色、渐变色和模式。另外，SVG 还定义有剪切、遮罩、组合等操作，以及滤镜操作。SVG 提供的这些机制，足够用来描述复杂的二维图形，结合后面讲述的动画机制，还可以用来展现复杂的二维动画。  

## 5.4  动画  

计算机中常见的动画，主要使用两种方法表述。第一种基于静止图像，如前述 GIF89a 格式；第二种基于矢量图形表述，如 Flash 动画或者 SVG 动画。  

### 5.4.1  基于静止图像  

使用 GIF89a 格式表述的动画，本质上定义了一系列要连续播放的动画帧，每一帧对应一副图像；当我们快速展示不同的静止图像时，给人脑的感觉就是连续的。GIF89a 定义的最短切换时间为 10 毫秒，也就是说，最高可达到每秒切换 100 副静止图像的速度，而对人类来讲，每秒 25 帧即可达到较为平滑的动画播放视觉效果。  

GIF89a 这个格式的出现距今已有三十年的时间了，但仍然被广泛使用。比如我们现在使用微博、微信时，其中有很多动画表情（俗称“动图”）采用的就是 GIF89a 格式。另外，如前所述，我们还可以将一段视频转换为 GIF89a 格式保存并播放，但由于 GIF 每一帧图像只能包含 256 种颜色，故而会导致色彩出现较为严重的失真现象。  

类似地，我们也可以用一组连续的 JPEG 图片形成一个动画文件，这就是 Motion JPEG，简称 MJPEG。MJPEG 中的每一帧图像使用 JPEG 格式压缩，常用于数码相机等移动设备中，用来记录动画短片。MJPEG 的另外一个常见应用场合是视频录制器（Video Recorder），这种设备可将模拟或数字电视的视频信号逐帧压缩为 JPEG，并最终保存为 MJPEG。因为 MJPEG 采用的是帧内压缩技术，相邻两帧之间的数据没有关联，因此，可做逐帧的视频编辑，并广泛应用于视频的非线性编辑。也正因为此特点，在使用 MJPEG 时，无法在画面基本不发生变化（如新闻播报节目）的情形下获得更高的压缩率，故而在视频播放领域，主要使用本章后面讲到的 MPEG 等压缩技术。  

### 5.4.2  基于矢量图形  

相比上面基于静止图像的动画，基于矢量图形实现动画要复杂一些。不过其原理并不复杂。比如，我们要描述地球围绕太阳的公转运动（圆形物体的椭圆运动），只要描述出地球公转的椭圆轨迹就可以了。渲染动画的程序，根据设定的速度和椭圆的轨迹即可动态计算出地球的位置，并按照当前时间更新地球的位置即可实现动画效果。  

这种方法除了可以描述物体（对象）的运动轨迹之外，也可以用来描述对象的颜色、透明度、大小等的变化。  

SVG 1.1 版本增加了动画支持。清单 5-3 给出了一个简单的动画示例[^9]。  

<small>清单 5-3  SVG 动画示例</small>  

```
<?xml version="1.0" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" 
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg width="8cm" height="3cm"  viewBox="0 0 800 300"
     xmlns="http://www.w3.org/2000/svg" version="1.1">
  <desc>演示 animation 元素</desc>
  <rect x="1" y="1" width="798" height="298" 
        fill="none" stroke="blue" stroke-width="2" />
  <!-- 下面的代码描述了如何使用 animation 元素来定义一个动画。
        这些 animation 元素分别改变矩形的 x、y、width、height 四个属性，
        最终使之充满整个视口 -->
  <rect id="RectElement" x="300" y="100" width="300" height="100"
        fill="rgb(255,255,0)"  >
    <animate attributeName="x" attributeType="XML"
             begin="0s" dur="9s" fill="freeze" from="300" to="0" />
    <animate attributeName="y" attributeType="XML"
             begin="0s" dur="9s" fill="freeze" from="100" to="0" />
    <animate attributeName="width" attributeType="XML"
             begin="0s" dur="9s" fill="freeze" from="300" to="800" />
    <animate attributeName="height" attributeType="XML"
             begin="0s" dur="9s" fill="freeze" from="100" to="300" />
  </rect>
  <!-- 设立一个新的用户坐标系统，使得文本字符串的原点位于(0,0)，
        从而相当于新的原点旋转和放大字符串文本。-->
  <g transform="translate(100,100)" >
    <!-- 下面的代码演示了 set、animateMotion、
         animate 和 animateTransform 元素。text 元素最初
         是隐藏的（也就是不可见）。在第三秒时：
           * 变为可见
           * 沿视口的对角线连续移动
           * 其颜色从蓝色向深红色变化
           * 从 -30 度向零度（水平）旋转
           * 将字号放大三倍。-->
    <text id="TextElement" x="0" y="0"
          font-family="Verdana" font-size="35.27" visibility="hidden"  > 
      It's alive!
      <set attributeName="visibility" attributeType="CSS" to="visible"
           begin="3s" dur="6s" fill="freeze" />
      <animateMotion path="M 0 0 L 100 100" 
           begin="3s" dur="6s" fill="freeze" />
      <animate attributeName="fill" attributeType="CSS"
           from="rgb(0,0,255)" to="rgb(128,0,0)"
           begin="3s" dur="6s" fill="freeze" />
      <animateTransform attributeName="transform" attributeType="XML"
           type="rotate" from="-30" to="0"
           begin="3s" dur="6s" fill="freeze" />
      <animateTransform attributeName="transform" attributeType="XML"
           type="scale" from="1" to="3" additive="sum"
           begin="3s" dur="6s" fill="freeze" />
    </text>
  </g>
</svg>
``` 
<br> 

清单 5-2 中的 SVG 文档，首先定义了一个矩形，并通过其子元素 animation 定义了该矩形的四个属性：x、y、width、height 的变化，如：  

```
    <animate attributeName="x" attributeType="XML"
             begin="0s" dur="9s" fill="freeze" from="300" to="0" />
```  
<br>

上述代码表示，从 0 秒开始，在 9s 的时间内，使矩形的 x 值从 300 改变到 0。类似的，其他的 animation 元素定义了该矩形其他三个属性的变化情况。  

其后的代码用来控制一个文本字符串（It's alive！）的动画效果。如注释所言，text 元素最初是隐藏的（visibility="hidden"）。从第三秒开始，在六秒内：  

- 该元素变为可见，
- 该元素沿视口的对角线连续移动，
- 其颜色从蓝色向深红色变化，
- 该元素从 -30 度向零度（水平）旋转，
- 该元素的字号放大三倍。  

在定义 text 元素及其动画子元素之前，代码还通过 g 元素重新定义了一个用户坐标系，将原坐标系中的 (100,100) 设置为新坐标系的原点。  

图 5-8 中的四幅图，分别给出了上述 SVG 动画在初始时、三秒时、六秒时以及九秒时的效果。  

<div align="center">
  ![示例 SVG 动画在关键时间点上的效果](illustration/img-5-8.png)<br>
  图 5-8  示例 SVG 动画在关键时间点上的效果
</div>
<br>

在清单 5-2 给出的 SVG 动画中，矩形的大小、位置，文本的颜色、角度和字号，在其动画过程中是线性、匀速变化的。但在实际的动画效果中，鲜有这种线性和匀速的变化方式。比如，要实现一个真空中自由落体的物体，其下降轨迹显然要符合牛顿第二定律定义的速度。这时，动画运动轨迹（包括大小、速度等其他参数的变化）的描述，就可以采用前述的路径来表述复杂的运动轨迹。  

清单 5-4 给出了使用路径定义元素运动轨迹的示例[^10]。  

<small>清单 5-4  使用路径定义元素运动轨迹的 SVG 动画示例</small>  

```
<?xml version="1.0" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" 
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg width="5cm" height="3cm"  viewBox="0 0 500 300"
     xmlns="http://www.w3.org/2000/svg" version="1.1"
     xmlns:xlink="http://www.w3.org/1999/xlink" >
  <desc>演示运动和动画</desc>
  <rect x="1" y="1" width="498" height="298"
        fill="none" stroke="blue" stroke-width="2" />
  <!-- 绘制蓝色的运动路径（path1），包括其上三个小圆（分别在起始点、中点和结束点。-->
  <path id="path1" d="M100,250 C 100,50 400,50 400,250"
        fill="none" stroke="blue" stroke-width="7.06"  />
  <circle cx="100" cy="250" r="17.64" fill="blue"  />
  <circle cx="250" cy="100" r="17.64" fill="blue"  />
  <circle cx="400" cy="250" r="17.64" fill="blue"  />
  <!-- 这里定义了一个三角形，该三角形将沿运动路径移动。
       It is defined with an upright orientation with the base of
       the triangle centered horizontally just above the origin. -->
  <path d="M-25,-12.5 L25,-12.5 L 0,-87.5 z"
        fill="yellow" stroke="red" stroke-width="7.06"  >
    <!-- 定义沿运动路径（path1）的动画 -->
    <animateMotion dur="6s" repeatCount="indefinite" rotate="auto" >
       <mpath xlink:href="#path1"/>
    </animateMotion>
  </path>
</svg>
```
<br>  

清单 5-4 中前面的代码绘制了蓝色的圆弧和三个小圆，用来表示运动路径。之后的代码定义了一个黄色的三角形（也使用路径定义），然后通过三角形的 animationMotion 子元素定义了三角形的动画运动路径（使用 mpath 子元素指向标识符为 path1 的蓝色路径），该动画在六秒内（dur=6s）完成，并且无限重复（repeatCount="indefinite"）、自动旋转（rotate="auto"）。图 5-9 中的三幅图，分别给出了上述 SVG 动画在初始时、三秒时以及六秒时的效果。  

<div align="center">
  ![基于运动路径的 SVG 动画示例在关键时间点上的效果](illustration/img-5-9.png)<br>
  图 5-9  基于运动路径的 SVG 动画示例在关键时间点上的效果
</div>
<br>  

## 5.5  音频  

如前所述，声音（声波）经过麦克风拾取[^11]后变成音频信号。这时，音频信号是连续的模拟信号，但数字计算机无法处理模拟信号，因而需要经过量化（或数字化）处理。参照图 5-10，音频信号的量化处理包括两种情形：  

- 使用 ADC（Analog-to-Digital Converter，模数转换器）按固定的时间间隔对音频信号做采样，将声波在时间上做离散处理，见图 5-10 B。时间间隔决定了采样频率。
- 每个采样值本身也会被量化做离散处理，通常取 8 位、12 位、16 位、24 位、32 位等二进制离散数字表达，见图 5-10 C。显然，量化位数越多，音频信号的保真度也越高。  

<div align="center">
  ![音频信号的量化处理](illustration/img-5-10.png)<br>
  图 5-10  音频信号的量化处理
</div>
<br>  

显然，采样频率越高，音频信号的保真度越高；对采样值的量化位数越多，音频信号的保真度也越高。那么，对音频信号而言，采样频率和采样值位数取多少合适呢？  

这通常取决于不同的音频信号类型。人耳可以听到的声音频率范围在 20Hz 到 20KHz，其中有噪音、语音，也有音乐。比如语音而言，8000 Hz 的采样频率，8 位的采样值量化位数就可以达到电话质量（能听清，且基本能听出来对方是谁）。对音乐而言，采样频率和量化位数都要比较高才行，毕竟很多发烧友的耳朵比指挥家还灵敏，因此，一般使用 44100 Hz 的采样频率，16 位的量化位数记录音乐，这就是所谓的 CD 质量。值得注意的是，44100 Hz 这个采样频率约为人耳可以分辨的声音频率上限（20KHz）的两倍，这是有一定的理论基础的。  

如果我们将经过上述量化处理之后的数据原封不动地保存下来，就是音频信号的最原始数据了。用这种方式存储原始的音频量化数据的，以 Windows 平台上的 WAV 文件为典型。  

本质上，WAV 文件提供了音频信号的高保真无损存储，支持多种音频量化位数、采样频率和声道，但其缺点是文件体积较大。比如，如果我们要按照电话质量保存某领导半个小时的讲话，则其原始的音频量化数据大小为：8000 \* 60 * 30 = 14,400,000B，约为 13.4 MB；而如果对半个小时的交响乐演奏以 CD 质量、双声道（立体声）做量化处理，则原始数据大小为：44100 \* 2 * 60 \* 30 * 2 = 317,520,000，约为 302 MB。由于存储容量的持续增加以及互联网带宽的持续提升，这些数据的大小现在看起来不怎么令人惊讶。但是，如果你想在一个小型的音乐随声听里边放上 1,000 首歌曲，每首平均三分钟长占用 30MB 空间，那就需要 30GB 的存储空间；就算在今天，30GB 的存储空间对便携式数码产品也是非常昂贵的。或者想象本世纪初的那几年，互联网带宽大概在 64Kbps，要想下载一首三分钟长（30MB）的歌曲，就要等上十几分钟的时间，这基本上也是没法接受的。  

要解决这个问题，计算机工程师们首先想到的就是做数据压缩。音频量化数据的压缩有三种手段。  

第一种手段在采样阶段完成，因为可以完全还原原始数据，所以属于无损压缩。  

第二种手段类似我们常见的 ZIP/RAR 文件压缩方法，基于原始量化数据进行数据的压缩处理，所以也属于无损压缩，只是相应的压缩算法针对音频数据的特点做了特别设计，所以要比 ZIP 等通用压缩算法更加高效。这种手段的典型音频格式为 FLAC（Free Lossless Audio Codec，免费无损音频编解码器）。  

第三种手段通过离散傅里叶变换或者其他数字信号处理原理下的数学变换（如小波变换），将时域信号转换为频域信号而压缩数据，属于有损压缩。典型的就是大家熟知的 MP3，见本章 5.7.3 小节“音视频编码技术：MPEG”。  

本章我们重点看第一种手段。我们之前描述的将模拟音频信号进行量化处理的方法称为 PCM（Pulse Code Modulation，脉冲编码调制）。通常，经过 PCM 处理之后，我们存储的是采样点的绝对量化值，这通常需要一个完整的采样值量化位数来存储。但如果我们观察一个实际的音频波形的话，会发现相邻两个采样值之间的差相对较小；比如，对 8 位的量化位数（取值范围在 [-127, 128] 这个区间内）来讲，相邻两个采样值之间的差大部分会落在 [-16, 16] 这个区间范围内，前面这个区间的数值需要 8 位来存储，后后面这个区间的数值只需要 4 位存储就可以了。这样，我们就可以仅记录第一个采样点的量化数值，然后记录下一个采样点相当于上一个采样点的差就能降低存储空间的占用。这就是 DPCM（Differential Pulse Code Modulation，差分脉冲编码调制）的原理。  

如果进一步分析，我们还会发现有很多采样点的量化值并没有发生变化，也就是说，可能有很多相邻、连续的采样点的量化值是一样的，这个时候，我们就可以仅记录这些采样点的数量和差分值，而不需要完整记录每个采样点的差分值，进而进一步降低对存储空间的需求。这就是 ADPCM（Adaptive Difference Pulse Code Modulation，自适应差分脉冲编码调制）的原理。一般来讲，相比标准的 DPM，采用 DPCM 可使存储量减少约 25%，而 ADPCM可压缩更多的数据。  

第二、三种手段中使用的具体压缩算法，将在本书第三篇“信息的计算机处理”中讲述。在现实中，使用第三种手段最常见的就是 MP3 格式，使用 MP3 压缩一首三分钟的歌曲后，大概只需要 5MB 左右的存储空间。除了 MP3 格式之外，还有 Ogg Vorbis、Opus 等开源、无专利限制的音频压缩格式[^12]，以及后来出现的 AAC 格式。  

AAC（Advanced Audio Coding，高级音频编码）现在正在取代 MP3 成为新的音乐压缩标准。AAC 最大可容纳 48 通道的音轨，采样频率最高可达 96KHz，并且在 320Kbps 的数据速率下能为 5.1 声道（杜比环绕声音效）音乐节目提供非常高的还原品质。和 MP3 比起来，它的音质更好，还能节省大约 30% 的储存空间与带宽。  

## 5.6  MIDI  

我们知道，不论用什么乐器，我们都可以用简谱或者五线谱来记录音乐，其中包括节奏、音高、强弱等。使用不同的乐器演奏同一个的乐谱，虽然其音调和节奏是一样的，但其音色会有显著变化，而音色取决于每种乐器在演奏不同音符时所形成的声波之谐振波。因此，假如我们要对不同乐器演奏的乐曲做数字化处理，我们只要知道音高（也就是频率）、强度（亦即声波的振幅）、节奏（快慢），以及音色（对应的谐振波组成），就可以使用计算机的方法合成出对应的音乐。和矢量图形类似，我们可以将这种音乐的合成方式称为矢量音乐。和矢量图形类似，使用矢量化的音乐表述方式，可以大大节省存储空间，且理论上不存在失真问题。  

目前广泛使用的 MIDI（Musical Instrument Digital Interface，乐器数字接口）就是基于上述概念发明出来的。MIDI 最早出现于 20 世纪 80 年代，由美国加州的音乐人 Dave Smith 发明。  
 
MIDI是编曲界最广泛使用的音乐标准格式，可称为“计算机能理解的乐谱”。利用 MIDI，音乐家在家里就可以为大型交响乐团作曲。  

一般而言，使用 MIDI 作曲时，每个音色（乐器）对应一个音轨，作曲家为每个音轨定义其音高（频率）、强弱（音量）、节奏（时长）等数据，最终形成一个复杂的 MIDI 音乐。除了计算机支持 MIDI 之外，现在的很多电声乐器（如电钢琴），本质上也是MIDI 设备。  

MIDI 当中的不同音色有多种实现方法。一种方法就是利用前述原理，使用谐振波合成的方法来合成某种音色，缺点是音色的还原度低，毕竟特定乐器的谐振波组合是非常复杂的。较为直接的方法就是事先记录特定乐器之不同音符对应的音频信号，然后再根据每个音轨对应的音符强弱和时长合成在一起播放。这种方法是目前普遍采用的方法，主要优点是音色还原度高，另外还可根据情况替代或者更新某种音色（从而形成音色库）。这种方法称为“波表合成”。  

现代电子计算机（包括智能手机）中都普遍包含有独立的音频处理模块或者控制卡（在个人电脑上称为声卡），其中均包含有 MIDI 功能，可以直接播放 MIDI 音乐。MIDI 音乐通常存储为 MID、RMI 为扩展名的文件。  

## 5.7  视频  

计算机中的视频处理，和电视广播系统的视频技术发展密不可分。早先，电视系统中的视频信号是模拟的，主要有 NTSC 和 PAL 两种制式。后来，电视系统开始转向使用数字信号，通过使用数字化的视频信号，观众可以获得更高的画质（更高清晰度），甚至可用来传输三维视频内容。  

而随着个人电脑和智能手机的普及，越来越多的人开始使用计算机、平板电脑或者智能手机来观看视频内容，甚至可用智能手机拍摄自己的视频内容。本质上讲，计算机可播放的视频信息和数字电视所使用的视频信息没有差别。但在压缩技术、编码方式等方面要比数字电视复杂一些。另外，随着互联网的发展，计算机的视频内容还需要同时满足网络传输的要求，比如在给定的传输速率下，获得高清视频的流畅播放效果。  

相比音频来讲，视频数据在计算机中的表述方法要复杂很多。接下来我们分几个小节来阐述视频的计算机表述。  

### 5.7.1  视频相关概念  

首先是清晰度。我们在使用智能电视播放视频时，经常会看到标清、高清、超清等选项。显然，高清视频的视觉效果要超过标清视频，但需要更高的数据传输带宽才能流畅播放。  

视频的清晰度（definition）之定义和电视系统有关。和计算机屏幕的分辨率（resolution）稍有不同，电视使用水平扫描线的概念来定义垂直方向的分辨率。如前所述，电视系统传输视频信号时，使用 YUV 或者 YIQ 色彩空间。以 NTSC 使用的 YUV 色彩空间为例，视频信号被分成三部分，第一部分定义了每条水平扫描线上每个像素点的亮度，而第二部分和第三部分（色度部分），在每条水平扫描线上仅给出了一半像素点（每两个亮度像素点对应一个色度像素点）对应的值。这来源于电视信号从黑白向彩色的过渡：彩色信号（色度）信号被夹杂在原始的黑白电视信号中一并传输，彩色电视可正常接收色度信号并显示，但黑白电视可忽略色度信号而仅显示灰度图像。因此，和计算机屏幕的分辨率准确对应的是视频信号的亮度部分所定义的像素分辨率。  

以 NTSC 制式为准，每一帧图像由 480 条水平扫描线组成，每条水平扫描线上有 720、704 或者 640 个像素点；高清电视（HDTV，High Definition TV）的水平扫描线可达到 1080 条，每条扫描线上有 1920 个像素点，对应的屏幕分辨率为 1920x1080；超高清电视（Ultra HDTV，又称“4K电视”），对应的屏幕分辨率为 3840×2160。与之相关的概念是长宽比（aspect ratio），传统电视的长宽比为 4:3，而 HDTV 的长宽比为 16:9。现代的智能电视，其屏幕分辨率均为16:9，所以在全屏播放传统的 4:3 电视信号时，会出现人变“胖”的情形。  

另外一个概念是帧率。帧率定义了每秒钟切换静态图像的帧数，通常以 fps（frames per second）来表示。我们知道，要利用人眼的视觉暂留特性，最低要达到 10fps 的帧率才行。电影胶片一般每秒播放 24 帧静态图像，从而可以保证比较平滑的视觉效果。PAL 制式的电视对应的帧率为 25fps，NTSC 制式对应的帧率为 29.97fps，而最新的一些技术已经将帧率提升到了 120fps。显然，增加帧率可获得更加流畅的视觉效果，尤其在播放画面变化很快的视频时，比如在警匪片中经常看到的追车场面。另外，帧率越高，视频的数据量越大，需要更高的数据传输带宽才能流畅播放。  

还有一个概念和人们为了使用较低数据量达到较高清晰度而采用的技巧有关，即扫描方式。考虑到一个视频相邻两幅静态图像之间的差别通常较小，所以，如果我们让相邻两帧仅包含整个图像中的一半像素，则可以大大降低视频内容的数据量（一半），且视觉效果并不会差太多——人眼是最容易被欺骗的。这就是交错扫描（Interlaced Scan，又称隔行扫描）模式。在交错扫描模式下，第一帧静态画面包含序号为偶数的扫描线像素数据，而第二帧静态画面包含序号为奇数的扫描线像素数据。和交错扫描模式对应的就是逐行扫描（Progressive Scan）模式：所有静态画面中包含有所有扫描线的像素数据。这就是我们经常在智能电视铭牌上看到 1080p 或者 1080i 的区别；前者表示逐行扫描，后者表示交错或隔行扫描。逐行扫描和交错扫描在视觉上的效果还是比较明显的，使用隔行扫描时，观者会感觉图像有明显的闪烁，观看时间长了，会出现比较严重的视觉疲劳。需要注意的是，4K 超高清电视仅支持逐行扫描。  

### 5.7.2  视频压缩原理  

和音频类似，视频数据通常会通过压缩技术来降低数据量，否则经过简单的计算就可以知道一分钟的高清视频片段，如果不做任何压缩，则需要大概 8,898M 字节（约为 8GB）的存储空间，这是无法接受的。因此，几乎所有的视频数据都会经过不同方法的压缩，然后再进行存储或传输。最简单的压缩办法就是采用前述小节“动画”中讲述的 MJPEG 技术，即将每一帧静态图像利用 JPEG 技术进行压缩。另外，如果观察常见的视频画面，我们会发现很多视频画面的背景是不变的，典型的如播音员播报新闻的画面。更进一步说，相邻的静态图像之间会有很多共同之处，因此，我们可以利用这一特点在相邻的静态图像之间做压缩处理。首先，我们保存第一帧画面的完整图像，然后，对其后的每一帧，记录和前一帧的差异数据，类似音频处理中的差分脉冲编码。采用这种方法之后，将更进一步降低存储视频信息的数据量。  

在视频压缩技术中，前一种方法称为“帧内压缩（intraframe compression）”，后一种方法称为“帧间压缩（interframe compression）”。我们将在本书第三篇“信息的计算机处理”中详细阐述视频压缩技术。  

根据视频内容的清晰度、帧率以及所采取的压缩技术，我们可以确定一个流畅展现一段视频内容所需要的最高传输速度。这里的传输速度指从硬盘、光盘或者网络上获取视频流数据然后播放的速度要求[^13]。这就是视频流数据对应的码率（bit rate），以每秒传输位数（bits per second，bps）为单位表示。需要注意的是，码率表示的是压缩率较低情况下所需要的最高传输速率，而不是最低传输速率。毕竟使用帧间压缩技术，在画面保持不变的情况下，所需要的传输速率可能降低到几乎为零。  

在网络视频播放环境下，视频码率对应的就是网络带宽，给出了流畅播放某个视频所要求的最高网络带宽。比如，播放分辨率为 1080p、帧率为 25 fps 的高清视频，尖峰时候的网络带宽要求大概为 10 Mbps。  

### 5.7.3  MPEG  

MPEG（Moving Picture Experts Group，动态图像专家组）是 ISO 与 IEC[^14] 于1988年成立的专门针对运动图像和语音压缩制定国际标准的组织。在已有的 JPEG 标准以及 H.261 视频编码技术基础上，MPEG 最初的工作于 1993 年被接受为 ISO/IEC11172 标准，这就是我们常说的 MPEG-1。  

MPEG-1 主要针对当时的 VCD 光盘和电视清晰度制定了对应的运动图像编码技术。MPEG-1 定义视频码率不超过 1.2 Mbps，而像素分辨率不超过 768x576，支持 1:1、4:3 和 16:9 多种长宽比，还支持从 23.976 Hz 到 60 Hz 间的八种帧率。另外，MPEG-1 也同时支持音频编码，所支持的音频码率为 32 到 448 Kbps 之间。MPEG-1 标准的颁布对后来的 VCD 播放机及 MP3 便携式随身听的市场发展起到了决定性作用。  

#### *1) MPEG-1 视频编码*

MPEG-1 定义的图像，使用类似于 YUV 的色彩空间；每一帧图像由三个部分组成，第一部分定义亮度，第二、第三部分定义色度，且亮度部分像素点数量在水平和垂直方向是第二、第三部分的两倍。  

MPEG-1 结合前述的帧内压缩技术和帧间压缩技术。MPEG-1 采用 JPEG 技术实现帧内压缩，为了获得更高的压缩比，使用了基于时间的静态图像预测技术来实现帧间压缩。MPEG-1 的数据流中包含如下几种图像编码类型：  

- I-Frame（Intra-coded Image，内编码图像）。I-Frame 使用 JPEG 压缩技术，是自包含的，无需引用其他图像内容。
- P-Frame（Predictive-coded Image，预测编码图像）。如其名称所暗示，P-Frame 记录的是其相对于之前的 I-Frame（以及其他前置 P-Frame）的变化部分，因此，在编码和解码的过程中，要依赖于前一个 I-Frame 及所有的前置 P-Frame。
- B-Frame（Bi-directionally Predictive-coded Image，双向预测编码图像）。如其名称所暗示，B-Frame 被定义为前一张静态图像和后续 I-Frame 或 P-Frame 之间的差异。显然，B-Frame 需要前后各一个 I-Frame 或者 P-Frame 图像才能正确编解码。假想一只球在一个静态背景上从左向右移动，左侧被球遮盖的部分图像包含在后续的图像中。通过 B-Frame，可通过预测后续图像而不是前置图像而获得。这就是使用 B-Frame 的意义。  

在实际的 MPEG-1 数据流中，使用上述不同的编码类型的图像帧是周期性交替出现的，如图 5-11 所示。  

<div align="center">
  ![MPEG-1 的图像编码类型](illustration/img-5-11.png)<br>
  图 5-11  MPEG-1 的图像编码类型
</div>
<br>  

周期性交替使用不同的图像编码类型，是为了在压缩率和随机定位、前进、后退等常见使用场景下获得一个良好的平衡：  

- 大量使用 I-Frame，将导致压缩率过低；
- 大量使用 P-Frame 或者 B-Frame，压缩率变高，但无法在视频流的随机定位、前进、后退等使用场景下尽快重建完整的静态图像。另外，由于使用有损压缩技术，大量使用 P-Frame 或者 B-Frame，还会使图像发生失真或者扭曲变形。  

故而，在实践中，MPEG-1 的不同类型图像编码以下面的顺序出现：IBBPBBPBB IBBPBBPBB …。这样，视频流的随机访问最小分辨率将是九张静态图像，当帧率为 30Hz 时，大概为 330 毫秒，这是可以接受的。  

我们提及视频时，通常强调的是运动的图像，却容易忽视视频中包含有可和运动图像同步播放的音频信号这一事实。所以，视频的编码技术，同时需要考虑音频。  

#### *2) MPEG-1 音频编码*  

MPEG-1 定义的音频编码使用三种采样率：44.1KHz[^15]、48KHz[^16] 和 32KHz，均为 16 位。根据编解码器的复杂性和性能，MPEG-1 定义了三层实现，见图 5-12。  

<div align="center">
  ![MPEG-1 的音频编码步骤](illustration/img-5-12.png)<br>
  图 5-12  MPEG-1 的音频编码步骤
</div>
<br>  

未压缩的音频数据使用 PCM 编码，可经过快速傅里叶变换（FFT）将其从离散的时域信号变换为离散的频域信号。经过 FFT 过滤后，离散频域信号被划分为不重叠的 32 个子频信号，分别记录每个子频信号的振幅。另外，通过调整音质模型，可确定每个子频信号的噪声级别；噪声级别高时，执行低分辨率量化，噪声级别低时，执行高分辨率量化。  

对于未压缩的音频数据（Layer-1，第一层），使用 PCM 编码，经 FFT 过滤和音质模型调整后的音频数据（Layer-2，第二层），也使用 PCM 编码，而经过第三层处理后的音频数据，使用霍夫曼编码（一种无损压缩技术）。  

另外，MPEG-1 所定义的音频编码，可支持单声道和立体声，而立体声可以使用两个独立的数据流来定义，也可以使用单个数据流来定义。前者也称为双声道（dual channel），后者称为混合立体声（joint stereo）。  

在编码音频数据时，MPEG-1 的每一层可使用 16 个固定的码率，从 32 Kbps 到 448 Kbps 不等。第一层和第二层不支持变化的码率，而第三层可支持变化的码率。  

MPEG-1 第三层就是我们熟知的 MP3 音乐压缩格式。如前所述，MP3 的音频数据是经过有损压缩和无损压缩两种技术的混合压缩数据。  

#### *3) MPEG-1 数据流*  

MPEG-1 定义了音频和视频的交错数据流语法。  

一个语音数据流由帧组成，而帧由音频访问单元组成。每个音频访问单元由多个槽（slot）组成。在第一层，一个槽包含四个字节，而在第二层和第三层，一个槽包含一个字节。每个音频访问单元是可被独立解压并播放的最小数据单元。一个帧中始终包含固定数量的采样点。比如在 48 KHz 的码率下，一帧的播放时间为 8 毫秒；32 KHz 情况下一帧的播放时间为 12 毫秒。见图 5-13。  

<div align="center">
  ![MPEG-1 音频数据流](illustration/img-5-13.png)<br>
  图 5-13  MPEG-1 音频数据流
</div>
<br>  

视频数据流可分成六个层来看待：  

1) 序列层（Sequence Layer）指构成某路节目的图像序列，序列起始码后的序列头中包含了图像尺寸、宽高比、帧率等信息。序列扩展中包含了一些附加数据。为保证能随时进入图像序列，序列头是重复发送的。
2) 序列层之下是图像组层（Group of Pictures Layer），一个图像组由相互间有预测和生成关系的一组I-Frame、P-Frame、B-Frame 图像构成，但头一帧图像总是I-Frame。图像组层的头部中包含了时间信息。
3) 图像组层下是图像层（Picture Layer），其中包含了 I-Frame、P-Frame 及 B-Frame。在其头部中包含了图像编码的类型和时间参考信息。
4) 图像层下是像条层（Slice Layer），一个像条包括了一定数量的宏块，其顺序与扫描顺序一致。宏块（Macro Block）是采用余弦变换（DCT）执行有损压缩时的最小处理单元，在 MPEG-1 中，一个静态图像按 16x16（亮度部分）或者 8x8（色度部分）划分成一个个的宏块。
5) 像条层下是宏块层（Macro Block Layer）。MPEG-1 中仅定义了 4\:2:2 一种宏块结构，而下面要讲到的 MPEG-2定义了三种宏块结构：4\:2:0、4\:2:2、4\:4:4。这个比例表示构成一个宏块的亮度像块和色差像块的数量关系。4\:2:0 宏块中包含四个亮度像块，一个 Cb 色差像块和一个 Cr 色差像块；4\:2:2 宏块中包含四个亮度像块，二个 Cb 色差像块和二个 Cr 色差像块；4\:4:4 宏块中包含四个亮度像块，四个 Cb 色差像块和四个 Cr 色差像块。
6) 宏块层下是像块层（Block Layer)。如上所述，像块层定义了宏块中的亮度和色差数据块。  

<div align="center">
  ![MPEG-1 的视频数据流](illustration/img-5-14.png)<br>
  图 5-14  MPEG-1 的视频数据流
</div>
<br> 

图 5-14 给出了 MPEG-1 的视频数据量分层解析。  

#### *4) MPEG-2*  

MPEG-2 制定于 1994 年，设计目标是高级工业标准的图象质量以及更高的码率。MPEG-2所能提供的码率在 3-10 Mbps 间，其在 NTSC 制式下的分辨率可达 720X486，在 MPEG-1 基础上，MPEG-2 的音频编码可提供 5.1 声道[^17]和多达七个伴音声道（DVD 可有八种语言配音）。  

众所周知，MPEG-2 标准颁布之后，DVD 播放器快速取代了 VCD 播放器。由于MPEG-2在设计时的巧妙处理，使得大多数 DVD 播放器也可播放 VCD 盘片。  

在音频方面，MPEG-2 在 MPEG-1 的基础上增加了低采样频率，有 16 KHz、22.05 KHz 以及 24 KHZ，并支持 5.1 声道和七个伴音声道，此外，MPEG-2 还增加了对前述 AAC 音频编码技术的支持（其采样频率可以低至8 KHz、高至96 KHz，其最多可支持 48 个通道）。  

在视频方面，MPEG-2 的视频编码方法并未发生本质的改变，但提供了更高品质的数字视频支持，包括更高的清晰度和更快的帧率。另外，MPEG-2 作为 MPEG-1 兼容性扩展，它提供了交错扫描视频的支持以及一些高级特性，如缩放、更丰富的颜色空间格式等。  

#### *5) MPEG-4*  

MPEG-4 于 2000 年成为国际标准。  

MPEG-4 与 MPEG-1 和 MPEG-2 有很大的不同。MPEG-4 不只是具体压缩算法，它是针对数字电视、交互式绘图应用（影音合成内容）、交互式多媒体（WWW、资料聚合与分发）等的整合，以及新的压缩技术的需求而制定的国际标准。  

MPEG-4 标准同以前标准的最显著的差别在于，它采用了基于对象的编码理念，即在编码时将一幅景物分成若干在时间和空间上相互联系的视频音频对象，分别编码后，再经过复用传输到接收端，然后再对不同的对象分别解码，从而组合成所需要的视频和音频。这样既方便我们对不同的对象采用不同的编码方法和表示方法，又有利于不同数据类型间的融合，并且这样也可以方便地实现对于各种对象的操作及编辑。例如，我们可以将一个卡通人物放在真实的场景中，或者将真人置于一个虚拟的演播室里，还可以在互联网上方便地实现交互，根据自己的需要有选择地组合各种视频音频以及图形文本对象。我们现在在大型演艺会场中经常看到的全息技术，就是上述理念的成功运用。  

与 MPEG-1、MPEG-2 相比，MPEG-4 具有如下独特的优点：  

- 基于内容的交互性。MPEG-4提供了基于内容的多媒体数据访问工具，如索引、超级链接、上传下载、删除等。利用这些工具，用户可以方便地从多媒体数据库中有选择地获取自己所需的内容，通过内容的操作和码流编辑功能，可应用于交互式家庭购物，淡入淡出的数字化效果等。MPEG-4 提供了高效的自然或合成的多媒体数据编码方法，它可以把自然场景或对象组合起来成为合成的多媒体数据。  
- 高效的压缩性。MPEG-4 基于更高的编码效率。同 MPEG-1/2 标准相比，在相同的码率下，可获得更高的视觉听觉质量，这使得在低带宽的信道上传送视频、音频成为可能。同时 MPEG-4 还能对多路数据流进行编码。一个场景的多视角或多声道数据流可以高效、同步地合成为最终的数据流。这可用于虚拟现实、三维电影、飞行仿真练习等。  
- 通用的访问性。MPEG-4 提供了易出错环境下的容错性，从而可以保证某些非可靠通讯环境下（如无线网络）中的应用。此外，MPEG-4 还支持基于内容的分级特性，即把内容、质量、复杂性分成许多不同级别来满足不同用户的不同需求，从而可在具有不同带宽、不同存储容量的传输信道和接收端展现不同的内容质量。  

读者应该会记得，在 MPEG-4 标准颁布之后的本世纪前十年，MP4 播放器曾在祖国大地上红极一时。  

### 5.7.4  其他视频编码技术  

#### *1) H.264*  

国际上制定视频编解码技术的组织有两个：一个是国际电信联盟，它制定的标准有 H.261、H.263、H.263+ 等；另一个是国际标准化组织，它制定的标准有 MPEG-1、MPEG-2、MPEG-4 等。而 H.264 则是由这两个组织共同组建的联合视频组（JVT，joint video team）制定的数字视频编码标准，所以它既是ITU-T的 H.264，又是 ISO/IEC 的MPEG-4 高级视频编码（Advanced Video Coding，AVC）的第 10 部分。因此，不论是MPEG-4 AVC、MPEG-4 Part 10，还是 ISO/IEC 14496-10，都是指 H.264。  

相比 MPEG-1/2/4，H.264 的优势有：  

1) 低码率。和 MPEG-2 和 MPEG4 ASP 等压缩技术相比，在同等图像质量下，采用 H.264 技术压缩后的数据量只有 MPEG-2 的 1/8，MPEG-4 的 1/3。  
2) 高质量的图像。H.264 能提供连续、流畅的高质量图像（DVD质量）。
3) 容错能力强。H.264 提供了解决在不稳定网络环境下容易发生的丢包等错误的必要工具。
4) 网络适应性强。H.264 提供了网络抽象层，使得 H.264 的文件可方便地在不同的网络上传输（例如互联网和无线通信网络等）。  

H.264 最大的优势是具有很高的数据压缩比率。在同等图像质量的条件下，H.264 的压缩比是 MPEG-2 的 2 倍以上，是 MPEG-4 的 1.5~2 倍。举个例子，原始文件的大小如果为 88 GB，采用 MPEG-2 压缩标准压缩后变成 3.5 GB，压缩比为 25:1，而采用 H.264后变为 879 MB，从 88 GB到 879 MB，H.264 的压缩比达到惊人的 102:1。正因为如此，经过 H.264 压缩的视频数据，在网络传输过程中所需要的带宽更少，从而也更加经济。  

随着 2010 年苹果公司宣布在 Safari 浏览器中支持 H.264 而不再支持 Flash 技术，H.264 几乎取代了 MPEG-4 成为 HTML5 Web 开发中的标准视频编码技术。  

#### *2) H.265*   

作为 H.264 的后续演进版本，H.265保留了原来的某些技术，同时对一些相关的技术加以改进。新技术使用先进的技术用以改善码流、编码质量、延时和算法复杂度之间的关系，以达到最优化。2012 年 8 月，爱立信公司推出了首款 H.265 编解码器，而在仅仅六个月之后，国际电联就正式批准通过了 HEVC/H.265 标准，标准全称为高效视频编码（High Efficiency Video Coding），相较于之前的 H.264 标准有了相当大的改善。值得一提的是，华为公司拥有最多的核心专利，是目前该标准的主导者。  

在即将到来的 4K 视频中，相信 H.265 将成为主流的视频编码技术。  

### 5.7.5  常见音视频格式  

从上个小节的介绍中我们可以看出，某些标准同时定义音视频编码技术，而有些标准仅定义视频或者音频。而在现实中，视频内容中往往同时包含有音频数据。除了音视频数据之外，复杂的多媒体数据流中还需要包含歌词、字幕（多种语言）、章节定义等等内容。在知识产品保护得到极大重视的今天，数据流中还需要包含数字版权认证信息。故而在实践中，出现了多种不同的音视频格式。有些格式由 MPEG 等标准定义，而有些格式是在这些标准形成之前，由操作系统或者软件平台厂商自己定义的，有些使用了自己的音视频编码技术。表 5-1 给出了常见的音视频格式。  

<small>表 5-1  常见音视频格式</small>  

| 后缀名   | 说明        |
|:---------|:------------|
| MP3      | 使用 MPEG-1 Layer 3 编码技术的音频文件，主要用于音乐、歌曲等。 |
| MPG/MPEG | 使用 MPEG-2 编码技术的视频文件。在早期的 VCD 盘片上，使用 MPEG-1 编码技术的视频文件通常具有 .DAT 后缀名。 |
| MP4      | 使用 MPEG-4 编码技术的音频或者视频文件。 |
| AAC      | 使用 AAC 编码技术的音频文件。 |
| OGG	   | Ogg 是一种容器格式，可封装由 Xiph.Org 基金会维护的各种开放、免费的音视频编码数据流。 |
| AVI、ASF、WMV、WMA |这三种格式是微软公司为 Windows 操作系统开发的音视频格式。WMV/WMA 是目前 Windows 平台上的主流格式，分别用于视频和音频。 |
| MOV      | 这是苹果公司为 QuickTime 多媒体平台开发的视频格式。 |
| FLV、F4V | 这两种格式是 Adobe 公司为 Flash 开发的视频流格式，主要用于在 Flash 中嵌入在线视频。F4V 采用 MPEG-4 视频编码技术。 |
| RM、RMVB	| 这两种格式由 Real Networks 公司开发，在网络带宽较小的年代曾红极一时。 |
| DivX     | DivX 原是由一群黑客为打破美国的 MPEG-4 技术禁运政策而发明的。最初 DivX 使用 MPEG-4 视频编码技术和 MPEG-1 Layer 3 音频编码技术来封装DVD 影片内容，由于大大缩小了文件体积而迅速走红互联网。DivX 现在已发展成为一种数字视频格式，支持MPEG-4、H.264和最新的 H.265 标准的视频，分辨率可高达 4K 超高清。 |
| WebM     | 这是由谷歌发展的一种音视频容器格式。其中的视频编码技术采用谷歌自己的 VP8，而音频编码技术采用 Ogg Vorbis。 |  

<br>
[^1]: 在计算机屏幕上显示的图像是否接近真实情况，还和显示屏的显示技术有关，有时同样类型的显示屏，因为电子元器件的漂移，也会带来明显的色差。  

[^2]: 此图取自维基百科“色彩带”词条。  

[^3]:【维基百科】在早期，GIF 文件所用的 LZW 压缩算法是 CompuServ 所开发的一种免费算法。然而令很多软件开发商感到意外的是，GIF 文件所采用的压缩算法忽然成了 Unisys 公司的专利。据 Unisys 公司称，他们已注册了 LZW 算法中的 W 部分的专利，如果要开发生成（或显示）GIF 文件的程序，则需向该公司支付版税。目前，GIF 相关专利已经失效，但在专利失效前曾引起部分开放源代码社区发起“Burn all GIFs”的运动抵制使用 GIF 格式。因此，人们开始寻求一种新技术，以减少开发成本。PNG标准就在这个背景下出现。

[^4]: [http://libmng.com/pub/png/libpng.html](http://libmng.com/pub/png/libpng.html)

[^5]: [http://libjpeg.sourceforge.net/](http://libjpeg.sourceforge.net/)

[^6]: [http://www.openjpeg.org/](http://www.openjpeg.org/)

[^7]: 该示例取自维基百科“SVG”词条。

[^8]: 该示例来自于 SVG 1.1 规范描述文档（[http://www.w3.org/TR/SVG11/paths.html#PathDataCurveCommands](http://www.w3.org/TR/SVG11/paths.html#PathDataCurveCommands0）。  

[^9]: 该示例来自于 SVG 1.1 规范描述文档（[http://www.w3.org/TR/SVG11/animate.html#AnimateMotionElement](http://www.w3.org/TR/SVG11/animate.html#AnimateMotionElement)）。略有修改。  

[^10]: 该示例来自于 SVG 1.1 规范描述文档（[http://www.w3.org/TR/SVG11/animate.html#AnimateMotionElement](http://www.w3.org/TR/SVG11/animate.html#AnimateMotionElement)）。略有修改。   

[^11]: 故而麦克风也经常被称作“拾音器”。  

[^12]: 这些开源、无专利的音视频编码技术（包括 FLAC），现在由 Xiph.Org 基金会管理，其官方网站为 [http://xiph.org/](http://xiph.org/) 。  

[^13]: 这里我们忽略了对视频数据进行解码和解压缩的过程所需要花费的时间。  

[^14]: International Electrotechnical Commission，国际电工委员会。  

[^15]: 即 CD-DA（Compact Disc Digital Audio，光盘数字音频）采样率。

[^16]: 即 DAT（Digital Audio Tap，数字音频磁带）采样率。

[^17]: 左右中、两个环绕声道以及一个加重低音声道。  
